{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14d08af",
   "metadata": {},
   "source": [
    "# Research into Chatbot stuff + Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3649de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: gpt-oss:20b at localhost:11434\n"
     ]
    }
   ],
   "source": [
    "import os, asyncio, datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import shutil\n",
    "\n",
    "import textwrap\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env.local\")\n",
    "\n",
    "# OLLAMA_MODEL = \"gpt-oss-20b\"\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\")\n",
    "OLLAMA_PORT = os.getenv(\"OLLAMA_PORT\")\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\")\n",
    "\n",
    "BASE_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}\"\n",
    "\n",
    "print(f\"Using Ollama model: {OLLAMA_MODEL} at {OLLAMA_HOST}:{OLLAMA_PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) LLM - Configure to include raw response with thinking\n",
    "llm = ChatOllama(\n",
    "    model=OLLAMA_MODEL, \n",
    "    base_url=BASE_URL,\n",
    "    temperature=0.7,\n",
    "    # Try to enable thinking/reasoning mode\n",
    "    # Some models need specific system prompts or parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333ec16",
   "metadata": {},
   "source": [
    "## Model Information\n",
    "\n",
    "**Important:** To see `<thinking>` tags, you need a model that supports reasoning tokens, such as:\n",
    "- `deepseek-r1` or `deepseek-r1:latest`\n",
    "- `qwen2.5` with thinking enabled\n",
    "- Other reasoning-capable models\n",
    "\n",
    "The `gpt-oss:20b` model may not expose thinking tokens by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65e7848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Prompt with history\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are concise and helpful.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f428bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Memory store (per session_id)\n",
    "_store: dict[str, InMemoryChatMessageHistory] = {}\n",
    "\n",
    "\n",
    "def get_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    return _store.setdefault(session_id, InMemoryChatMessageHistory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7682c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Chain with message history\n",
    "chain = RunnableWithMessageHistory(\n",
    "    prompt | llm,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d41d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(session_id: str, text: str) -> str:\n",
    "    resp = chain.invoke(\n",
    "        {\"input\": text},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    return resp.content\n",
    "\n",
    "\n",
    "def reset(session_id: str):\n",
    "    _store.pop(session_id, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f869add",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99e3ae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iâ€™m ChatGPT, a large language model created by OpenAI. I can help answer questions, explain topics, or assist with many tasks you have.\n",
      "Hello, Peter! ðŸ‘‹ How can I help you today?\n",
      "Hello, Peter! ðŸ‘‹ How can I help you today?\n",
      "You told me that your name is Peter.\n",
      "You told me that your name is Peter.\n"
     ]
    }
   ],
   "source": [
    "sid = \"peter\"\n",
    "print(ask(sid, \"Hello, who are you?\"))\n",
    "print(ask(sid, \"Remember my name and greet me briefly. It's Peter\"))\n",
    "print(ask(sid, \"What did I tell you earlier?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd98d7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_thinking': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is 12 * 8?', additional_kwargs={}, response_metadata={}), AIMessage(content='96', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-11-10T02:13:07.7038998Z', 'done': True, 'done_reason': 'stop', 'total_duration': 881251600, 'load_duration': 141882200, 'prompt_eval_count': 89, 'prompt_eval_duration': 119679700, 'eval_count': 48, 'eval_duration': 602206700, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--65ab2876-1c36-4876-b356-a209fba38bd9-0', usage_metadata={'input_tokens': 89, 'output_tokens': 48, 'total_tokens': 137})]), 'peter': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello, who are you?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi! Iâ€™m ChatGPT, an AI language model created by OpenAI. Iâ€™m here to answer questions, help with tasks, and chat about a wide range of topics. How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-11-10T02:13:11.8611425Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1137879700, 'load_duration': 136058600, 'prompt_eval_count': 87, 'prompt_eval_duration': 62827000, 'eval_count': 77, 'eval_duration': 894326700, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--9b3e3917-9014-46ac-845b-97c0c71bd263-0', usage_metadata={'input_tokens': 87, 'output_tokens': 77, 'total_tokens': 164}), HumanMessage(content=\"Remember my name and greet me briefly. It's Peter\", additional_kwargs={}, response_metadata={}), AIMessage(content='Sure, Peter! Nice to meet you.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-11-10T02:13:13.089423Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1222972100, 'load_duration': 141375700, 'prompt_eval_count': 150, 'prompt_eval_duration': 60057100, 'eval_count': 88, 'eval_duration': 989588800, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--0d163050-6f10-4330-8da3-0b2f9ac0abdd-0', usage_metadata={'input_tokens': 150, 'output_tokens': 88, 'total_tokens': 238}), HumanMessage(content='What did I tell you earlier?', additional_kwargs={}, response_metadata={}), AIMessage(content='You told me your name is Peter. Thatâ€™s all Iâ€™ve recorded from our earlier chat.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-11-10T02:13:14.4394235Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1345273900, 'load_duration': 139061900, 'prompt_eval_count': 176, 'prompt_eval_duration': 45702300, 'eval_count': 98, 'eval_duration': 1102863900, 'model_name': 'gpt-oss:20b', 'model_provider': 'ollama'}, id='lc_run--6d8e4948-edd2-4a4f-9055-89119a918b09-0', usage_metadata={'input_tokens': 176, 'output_tokens': 98, 'total_tokens': 274})])}\n"
     ]
    }
   ],
   "source": [
    "print(_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32beca5",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc58707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Conversation: peter ============\n",
      "[21:14:53] HUMAN\n",
      "Hello, who are you?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[21:14:53] AI\n",
      "Iâ€™m ChatGPT, a large language model created by OpenAI. I can help answer questions, explain\n",
      "topics, or assist with many tasks you have.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[21:14:53] HUMAN\n",
      "Remember my name and greet me briefly. It's Peter\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[21:14:53] AI\n",
      "Hello, Peter! ðŸ‘‹ How can I help you today?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[21:14:53] HUMAN\n",
      "What did I tell you earlier?\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[21:14:53] AI\n",
      "You told me that your name is Peter.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "TERM_W = shutil.get_terminal_size((100, 20)).columns\n",
    "WRAP_W = max(60, min(120, TERM_W - 4))\n",
    "\n",
    "ROLE_TAGS = {\n",
    "    \"system\": \"SYSTEM\",\n",
    "    \"user\": \"USER\",\n",
    "    \"assistant\": \"ASSISTANT\",\n",
    "    \"tool\": \"TOOL\",\n",
    "}\n",
    "\n",
    "\n",
    "def _now():\n",
    "    return dt.datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "def format_block(role: str, content: str, message=None) -> str:\n",
    "    role = ROLE_TAGS.get(role.lower(), role.upper())\n",
    "    \n",
    "    # Extract thinking section if present\n",
    "    thinking_match = re.search(r'<thinking>(.*?)</thinking>', content, re.DOTALL)\n",
    "    thinking_text = \"\"\n",
    "    display_content = content\n",
    "    \n",
    "    if thinking_match:\n",
    "        thinking_text = thinking_match.group(1).strip()\n",
    "        # Remove thinking tags from main content\n",
    "        display_content = re.sub(r'<thinking>.*?</thinking>', '', content, flags=re.DOTALL).strip()\n",
    "    \n",
    "    result = []\n",
    "    border = \"â”€\" * min(WRAP_W, 80)\n",
    "    \n",
    "    # Add thinking section if present\n",
    "    if thinking_text:\n",
    "        thinking_wrapped = textwrap.fill(thinking_text, width=WRAP_W)\n",
    "        result.append(f\"[{_now()}] {role} (THINKING)\")\n",
    "        result.append(thinking_wrapped)\n",
    "        result.append(border)\n",
    "    \n",
    "    # Check for tool calls in the message\n",
    "    if message and hasattr(message, 'tool_calls') and message.tool_calls:\n",
    "        result.append(f\"[{_now()}] {role} (TOOL CALLS)\")\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_info = f\"Tool: {tool_call.get('name', 'unknown')}\"\n",
    "            if 'args' in tool_call:\n",
    "                tool_info += f\"\\nArgs: {json.dumps(tool_call['args'], indent=2)}\"\n",
    "            if 'id' in tool_call:\n",
    "                tool_info += f\"\\nID: {tool_call['id']}\"\n",
    "            result.append(textwrap.fill(tool_info, width=WRAP_W))\n",
    "        result.append(border)\n",
    "    \n",
    "    # Check for additional_kwargs that might contain tool info\n",
    "    if message and hasattr(message, 'additional_kwargs') and message.additional_kwargs:\n",
    "        if 'tool_calls' in message.additional_kwargs:\n",
    "            result.append(f\"[{_now()}] {role} (TOOL CALLS - RAW)\")\n",
    "            result.append(textwrap.fill(str(message.additional_kwargs['tool_calls']), width=WRAP_W))\n",
    "            result.append(border)\n",
    "    \n",
    "    # Add main content\n",
    "    if display_content:\n",
    "        wrapped = textwrap.fill(display_content, width=WRAP_W)\n",
    "        result.append(f\"[{_now()}] {role}\")\n",
    "        result.append(wrapped)\n",
    "        result.append(border)\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "\n",
    "def print_history(session_id: str):\n",
    "    hist = get_history(session_id)\n",
    "    print(\"\\n\" + \"=\" * 12 + f\" Conversation: {session_id} \" + \"=\" * 12)\n",
    "    for m in hist.messages:\n",
    "        # m is HumanMessage/AIMessage/SystemMessage/ToolMessage\n",
    "        print(format_block(m.type, m.content, message=m))\n",
    "    print(\"=\" * (26 + len(session_id)))\n",
    "\n",
    "\n",
    "print_history(sid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "09-15-2025_discord-meeting-transcriptor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
